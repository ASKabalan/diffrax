{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877f5751-82f9-4f09-b2eb-8c6042a3f140",
   "metadata": {},
   "source": [
    "# Neural CDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e9c19-28b5-44c9-9a01-0ec508f2ac6c",
   "metadata": {},
   "source": [
    "This example trains a [Neural CDE](https://arxiv.org/abs/1810.01367) to distinguish clockwise from counter-clockwise spirals.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "```bibtex\n",
    "@incollection{kidger2020neuralcde,\n",
    "    title={Neural Controlled Differential Equations for Irregular Time Series},\n",
    "    author={Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},\n",
    "    booktitle={Advances in Neural Information Processing Systems},\n",
    "    publisher={Curran Associates, Inc.},\n",
    "    year={2020},\n",
    "}\n",
    "```\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab19e7-1e40-4b9e-99ab-3aa74f6e5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dedc5d-dc07-4dd1-845f-1251bab4b32a",
   "metadata": {},
   "source": [
    "A neural CDE looks like\n",
    "\n",
    "$y(t) = y(0) + \\int_0^t f_\\theta(y(s)) dx(s)$\n",
    "\n",
    "Where $x$ is your data and $f_\\theta$ is a neural network.\n",
    "\n",
    "The right hand side is a matrix-vector product between them. What we are about to define is the $f_\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e916e0a-df54-4045-9b74-16e536e58000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size * data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            # Note the use of a tanh final activation function. This is important to\n",
    "            # stop the model blowing up. (Just like how GRUs and LSTMs constrain the\n",
    "            # rate of change of their hidden states.)\n",
    "            final_activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y).reshape(self.hidden_size, self.data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8851b-e418-47e8-856e-622266c78612",
   "metadata": {},
   "source": [
    "Now wrap up the whole CDE solve into a model.\n",
    "\n",
    "In this case we cap the neural CDE with a linear layer and sigomid, to perform binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9dc97-de5c-4905-9e59-3ca474aa9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    stepsize_controller: diffrax.AbstractStepSizeController\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey = jrandom.split(key, 3)\n",
    "        self.initial = eqx.nn.MLP(data_size, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(data_size, hidden_size, width_size, depth, key=fkey)\n",
    "        self.stepsize_controller = diffrax.IController()\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 1, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path.\n",
    "        #\n",
    "        # `control` is just the continuous-time path, parameterised by these\n",
    "        # coefficients, that will be used to drive the CDE.\n",
    "        #\n",
    "        # `term` wraps the vector field and control together. In this case the\n",
    "        # `.to_ode()` call says that we'd like to solve \\int f(y(s)) dx(s) as an ODE,\n",
    "        # by treating the right hand side as \\int f(y(s)) dx/ds ds.\n",
    "        #\n",
    "        # [Not doing this corresponds to using the control just like time in an ODE\n",
    "        # solver, e.g. the explicit Euler method becomes\n",
    "        #\n",
    "        # y(t_j) + f(y(t_j)) (x(t_{j+1}) - x(t_j)).\n",
    "        #\n",
    "        # It's not known whether it's generally better to use `.to_ode()` or not. Try\n",
    "        # both and find out what works best for you?]\n",
    "        #\n",
    "        # `solver` is then just the solver we're using, like usual. Note that it's the\n",
    "        # capital `Tsit5` not the lower-case `tsit5`, because we're using the more\n",
    "        # advanced API. (The lower-case `tsit5` is just a shortcut for\n",
    "        # `Tsit5(ODETerm(...))` for the common ODE case.)\n",
    "        control = diffrax.CubicInterpolation(ts=ts, coeffs=coeffs)\n",
    "        term = diffrax.ControlTerm(vector_field=self.func, control=control).to_ode()\n",
    "        solver = diffrax.Tsit5(term=term)\n",
    "\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            solver,\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            y0=y0,\n",
    "            dt0=None,\n",
    "            stepsize_controller=self.stepsize_controller,\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jnn.sigmoid(self.linear(solution.ys[-1]))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96c981-86ed-4594-afae-6c087523a47c",
   "metadata": {},
   "source": [
    "Toy dataset of spirals.\n",
    "\n",
    "We interpolate the samples with Hermite cubic splines with backward differences, which were introduced in [https://arxiv.org/abs/2106.11028](https://arxiv.org/abs/2106.11028). (And produces better results than the natural cubic splines used in the original neural CDE paper.)\n",
    "\n",
    "**Note the inclusion of time as a channel! This is a subtle point that is often accidentally missed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647fc5f-4eaf-4a06-83e6-034508fbbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, add_noise, *, key):\n",
    "    theta_key, noise_key = jrandom.split(key, 2)\n",
    "    length = 100\n",
    "    theta = jrandom.uniform(theta_key, (dataset_size,), minval=0, maxval=2 * math.pi)\n",
    "    y0 = jnp.stack([jnp.cos(theta), jnp.sin(theta)], axis=-1)\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0, 4 * math.pi, length), (dataset_size, length))\n",
    "    matrix = jnp.array([[-0.3, 2], [-2, -0.3]])\n",
    "    ys = jax.vmap(\n",
    "        lambda y0i, ti: jax.vmap(lambda tij: jsp.linalg.expm(tij * matrix) @ y0i)(ti)\n",
    "    )(y0, ts)\n",
    "    # VERY IMPORTANT:\n",
    "    # Remember to include time as a channel, as usual for neural CDEs.\n",
    "    ys = jnp.concatenate([ts[:, :, None], ys], axis=-1)\n",
    "    ys = ys.at[: dataset_size // 2, :, 1].multiply(-1)\n",
    "    if add_noise:\n",
    "        ys = ys + jrandom.normal(noise_key, ys.shape) * 0.1\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = jnp.zeros((dataset_size,))\n",
    "    labels = labels.at[: dataset_size // 2].set(1.0)\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657dffa-da73-4c3c-a722-14eff8ca7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f9c67-8797-480b-a56c-c30af3c7bd26",
   "metadata": {},
   "source": [
    "The main entry point. Try running `main()` to train the neural CDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b14eb-2486-4c1a-ac2a-ade54d6d8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=256,\n",
    "    add_noise=False,\n",
    "    batch_size=32,\n",
    "    lr=1e-2,\n",
    "    steps=20,\n",
    "    hidden_size=8,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, add_noise, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    def loss(model, ti, label_i, coeff_i):\n",
    "        pred = jax.vmap(model)(ti, coeff_i)\n",
    "        # Binary cross-entropy\n",
    "        bxe = label_i * jnp.log(pred) + (1 - label_i) * jnp.log(1 - pred)\n",
    "        bxe = -jnp.mean(bxe)\n",
    "        acc = jnp.mean((pred > 0.5) == (label_i == 1))\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, (ti, label_i, *coeff_i) in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i)\n",
    "        end = time.time()\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "\n",
    "    ts, coeffs, labels, _ = get_data(dataset_size, add_noise, key=test_data_key)\n",
    "    bxe, acc = loss(model, ts, labels, coeffs)\n",
    "    print(f\"Test loss: {bxe}, Test Accuracy: {acc}\")\n",
    "\n",
    "    # Plot results\n",
    "    sample_ts = ts[-1]\n",
    "    sample_coeffs = tuple(c[-1] for c in coeffs)\n",
    "    pred = model(sample_ts, sample_coeffs, evolving_out=True)\n",
    "    interp = diffrax.CubicInterpolation(sample_ts, sample_coeffs)\n",
    "    values = jax.vmap(interp.evaluate)(sample_ts)\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "    ax1.plot(sample_ts, values[:, 1], c=\"dodgerblue\")\n",
    "    ax1.plot(sample_ts, values[:, 2], c=\"dodgerblue\", label=\"Data\")\n",
    "    ax1.plot(sample_ts, pred, c=\"crimson\", label=\"Classification\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlabel(\"t\")\n",
    "    ax1.legend()\n",
    "    ax2.plot(values[:, 1], values[:, 2], c=\"dodgerblue\", label=\"Data\")\n",
    "    ax2.plot(values[:, 1], values[:, 2], pred, c=\"crimson\", label=\"Classification\")\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_zticks([])\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"y\")\n",
    "    ax2.set_zlabel(\"Classification\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"neural_cde.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 CUDA 1",
   "language": "python",
   "name": "python3cuda1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
