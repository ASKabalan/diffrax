{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031699e-5dd0-41a3-8276-2c1cac2238d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def lipswish(x):\n",
    "    return 0.909 * jnn.silu(x)\n",
    "\n",
    "\n",
    "class VectorField(eqx.Module):\n",
    "    scale: Union[int, jnp.ndarray]\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, hidden_size, width_size, depth, scale, *, key):\n",
    "        super().__init__()\n",
    "        scale_key, mlp_key = jrandom.split(key)\n",
    "        if scale:\n",
    "            self.scale = jrandom.uniform(\n",
    "                scale_key, (hidden_size,), minval=0.9, maxval=1.1\n",
    "            )\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y]))\n",
    "\n",
    "\n",
    "class ControlledVectorField(eqx.Module):\n",
    "    scale: Union[int, jnp.ndarray]\n",
    "    mlp: eqx.nn.MLP\n",
    "    control_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, control_size, hidden_size, width_size, depth, scale, *, key):\n",
    "        super().__init__()\n",
    "        scale_key, mlp_key = jrandom.split(key)\n",
    "        if scale:\n",
    "            self.scale = jrandom.uniform(\n",
    "                scale_key, (hidden_size, control_size), minval=0.9, maxval=1.1\n",
    "            )\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size * control_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "        self.control_size = control_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y])).reshape(\n",
    "            self.hidden_size, self.control_size\n",
    "        )\n",
    "\n",
    "\n",
    "class DifferentialEquation(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    vf: VectorField  # drift\n",
    "    cvf: ControlledVectorField  # diffusion\n",
    "    readout: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_size,\n",
    "        control_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        readout_size,\n",
    "        scale,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        initial_key, vf_key, cvf_key, readout_key = jrandom.split(key, 4)\n",
    "\n",
    "        self.initial = eqx.nn.MLP(\n",
    "            initial_size, hidden_size, width_size, depth, key=initial_key\n",
    "        )\n",
    "        self.vf = VectorField(hidden_size, width_size, depth, scale, key=vf_key)\n",
    "        self.cvf = ControlledVectorField(\n",
    "            control_size, hidden_size, width_size, depth, scale, key=cvf_key\n",
    "        )\n",
    "        self.readout = eqx.nn.Linear(hidden_size, readout_size, key=readout_key)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def _initial(self, ts, init):\n",
    "        t0 = ts[0]\n",
    "        t1 = ts[-1]\n",
    "        y0 = self.initial(init)\n",
    "        dt0 = 1.0\n",
    "        return t0, t1, y0, dt0\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def _readout(self, ys):\n",
    "        return jax.vmap(self.readout)(ys)\n",
    "\n",
    "    def __call__(self, ts, init, control, saveat):\n",
    "        vf = diffrax.ODETerm(self.vf)\n",
    "        cvf = diffrax.ControlTerm(self.cvf, control)\n",
    "        term = diffrax.MultiTerm((vf, cvf))\n",
    "        solver = diffrax.ReversibleHeun(term)\n",
    "\n",
    "        t0, t1, y0, dt0 = self._initial(ts, init)\n",
    "        sol = diffrax.diffeqsolve(solver, t0, t1, y0, dt0, saveat=saveat)\n",
    "        return self._readout(sol.ys)\n",
    "\n",
    "\n",
    "class NeuralSDE(eqx.Module):\n",
    "    diffeq: DifferentialEquation\n",
    "    initial_noise_size: int\n",
    "    noise_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.diffeq = DifferentialEquation(\n",
    "            initial_size=initial_noise_size,\n",
    "            control_size=noise_size,\n",
    "            hidden_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            readout_size=data_size,\n",
    "            scale=True,\n",
    "            key=key,\n",
    "        )\n",
    "        self.initial_noise_size = initial_noise_size\n",
    "        self.noise_size = noise_size\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def _setup(self, ts, key):\n",
    "        init_key, bm_key = jrandom.split(key, 2)\n",
    "        init = jrandom.normal(init_key, (self.initial_noise_size,))\n",
    "        control = diffrax.UnsafeBrownianPath(shape=(self.noise_size,), key=bm_key)\n",
    "        saveat = diffrax.SaveAt(ts=ts)\n",
    "        return ts, init, control, saveat\n",
    "\n",
    "    def __call__(self, ts, *, key):\n",
    "        return self.diffeq(*self._setup(ts, key))\n",
    "\n",
    "\n",
    "class NeuralCDE(eqx.Module):\n",
    "    diffeq: DifferentialEquation\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key):\n",
    "        super().__init__()\n",
    "        self.diffeq = DifferentialEquation(\n",
    "            initial_size=data_size + 1,\n",
    "            control_size=data_size,\n",
    "            hidden_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            readout_size=1,\n",
    "            scale=False,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    @eqx.filter_jit\n",
    "    def _setup(ts, ys):\n",
    "        ys = diffrax.linear_interpolation(\n",
    "            ts, ys, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "        init = jnp.concatenate([ts[0, None], ys[0]])\n",
    "        control = diffrax.LinearInterpolation(ts, ys)\n",
    "        saveat = diffrax.SaveAt(t1=True)\n",
    "        return ts, init, control, saveat\n",
    "\n",
    "    def __call__(self, ts, ys):\n",
    "        return self.diffeq(*self._setup(ts, ys))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def clip_weights(self):\n",
    "        leaves, treedef = jax.tree_flatten(\n",
    "            self, is_leaf=lambda x: isinstance(x, eqx.nn.Linear)\n",
    "        )\n",
    "        new_leaves = []\n",
    "        for leaf in leaves:\n",
    "            if isinstance(leaf, eqx.nn.Linear):\n",
    "                lim = 1 / leaf.out_features\n",
    "                leaf = eqx.tree_at(\n",
    "                    lambda x: x.weight, leaf, leaf.weight.clip(-lim, lim)\n",
    "                )\n",
    "            new_leaves.append(leaf)\n",
    "        return jax.tree_unflatten(treedef, new_leaves)\n",
    "\n",
    "\n",
    "def get_data(key):\n",
    "    bm_key, y0_key, drop_key = jrandom.split(key, 3)\n",
    "\n",
    "    mu = 0.02\n",
    "    theta = 0.1\n",
    "    sigma = 0.4\n",
    "\n",
    "    t0 = 0\n",
    "    t1 = 63\n",
    "    t_size = 64\n",
    "\n",
    "    def drift(t, y, args):\n",
    "        return mu * t - theta * y\n",
    "\n",
    "    def diffusion(t, y, args):\n",
    "        return 2 * sigma * t / t1\n",
    "\n",
    "    bm = diffrax.UnsafeBrownianPath(shape=(), key=bm_key)\n",
    "    solver = diffrax.euler_maruyama(drift, diffusion, bm)\n",
    "    y0 = jrandom.uniform(y0_key, (1,), minval=-1, maxval=1)\n",
    "    dt0 = 0.1\n",
    "    ts = jnp.linspace(t0, t1, t_size)\n",
    "    saveat = diffrax.SaveAt(ts=ts)\n",
    "    sol = diffrax.diffeqsolve(solver, t0, t1, y0, dt0, saveat=saveat)\n",
    "    ys = sol.ys\n",
    "\n",
    "    to_drop = jrandom.bernoulli(drop_key, 0.3, (t_size, 1))\n",
    "    ys = jnp.where(to_drop, jnp.nan, ys)\n",
    "\n",
    "    return ts, ys\n",
    "\n",
    "\n",
    "def make_dataloader(arrays, batch_size, loop, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "        if not loop:\n",
    "            break\n",
    "\n",
    "\n",
    "def loss(generator, discriminator, ts_i, ys_i, key, step=0):\n",
    "    batch_size, _ = ts_i.shape\n",
    "    key = jrandom.fold_in(key, step)\n",
    "    key = jrandom.split(key, batch_size)\n",
    "    fake_ys_i = jax.vmap(generator)(ts_i, key=key)\n",
    "    real_score = jax.vmap(discriminator)(ts_i, ys_i)\n",
    "    fake_score = jax.vmap(discriminator)(ts_i, fake_ys_i)\n",
    "    return jnp.mean(real_score - fake_score)\n",
    "\n",
    "\n",
    "@eqx.filter_grad\n",
    "def grad_loss(g_d, ts_i, ys_i, key, step):\n",
    "    generator, discriminator = g_d  # We differentiate just the first argument\n",
    "    return loss(generator, discriminator, ts_i, ys_i, key, step)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(\n",
    "    generator, discriminator, g_opt_state, d_opt_state, g_optim, d_optim, g_grad, d_grad\n",
    "):\n",
    "    g_updates, g_opt_state = g_optim.update(g_grad, g_opt_state)\n",
    "    d_updates, d_opt_state = d_optim.update(d_grad, d_opt_state)\n",
    "    generator = eqx.apply_updates(generator, g_updates)\n",
    "    discriminator = eqx.apply_updates(discriminator, d_updates)\n",
    "    discriminator = discriminator.clip_weights()\n",
    "    return generator, discriminator, g_opt_state, d_opt_state\n",
    "\n",
    "\n",
    "def main(\n",
    "    initial_noise_size=5,\n",
    "    noise_size=3,\n",
    "    hidden_size=16,\n",
    "    width_size=16,\n",
    "    depth=1,\n",
    "    generator_lr=2e-5,\n",
    "    discriminator_lr=1e-4,\n",
    "    batch_size=1024,\n",
    "    steps=10000,\n",
    "    steps_per_print=10,\n",
    "    dataset_size=8192,\n",
    "    seed=5678,\n",
    "):\n",
    "\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    (\n",
    "        data_key,\n",
    "        generator_key,\n",
    "        discriminator_key,\n",
    "        dataloader_key,\n",
    "        train_key,\n",
    "        evaluate_key,\n",
    "        sample_key,\n",
    "    ) = jrandom.split(key, 7)\n",
    "    data_key = jrandom.split(data_key, dataset_size)\n",
    "\n",
    "    ts, ys = jax.vmap(get_data)(data_key)\n",
    "    _, _, data_size = ys.shape\n",
    "\n",
    "    generator = NeuralSDE(\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        key=generator_key,\n",
    "    )\n",
    "    discriminator = NeuralCDE(\n",
    "        data_size, hidden_size, width_size, depth, key=discriminator_key\n",
    "    )\n",
    "\n",
    "    g_optim = optax.rmsprop(generator_lr)\n",
    "    d_optim = optax.rmsprop(-discriminator_lr)\n",
    "    g_opt_state = g_optim.init(eqx.filter(generator, eqx.is_array))\n",
    "    d_opt_state = d_optim.init(eqx.filter(discriminator, eqx.is_array))\n",
    "\n",
    "    trange = tqdm.tqdm(range(steps))\n",
    "    infinite_dataloader = make_dataloader(\n",
    "        (ts, ys), batch_size, loop=True, key=dataloader_key\n",
    "    )\n",
    "\n",
    "    for step, (ts_i, ys_i) in zip(trange, infinite_dataloader):\n",
    "        g_grad, d_grad = grad_loss((generator, discriminator), ts_i, ys_i, key, step)\n",
    "        generator, discriminator, g_opt_state, d_opt_state = update(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            g_opt_state,\n",
    "            d_opt_state,\n",
    "            g_optim,\n",
    "            d_optim,\n",
    "            g_grad,\n",
    "            d_grad,\n",
    "        )\n",
    "\n",
    "        if (step % steps_per_print) == 0 or step == steps - 1:\n",
    "            total_score = 0\n",
    "            num_batches = 0\n",
    "            for ts_i, ys_i in make_dataloader(\n",
    "                (ts, ys), batch_size, loop=False, key=evaluate_key\n",
    "            ):\n",
    "                score = loss(generator, discriminator, ts_i, ys_i, sample_key)\n",
    "                total_score += score.item()\n",
    "                num_batches += 1\n",
    "            trange.write(f\"Step: {step}, Loss: {total_score / num_batches}\")\n",
    "\n",
    "    # Plot samples\n",
    "    fig, ax = plt.subplots()\n",
    "    num_samples = min(50, dataset_size)\n",
    "    ts_to_plot = ts[:num_samples]\n",
    "    ys_to_plot = ys[:num_samples]\n",
    "\n",
    "    def _interp(ti, yi):\n",
    "        return diffrax.linear_interpolation(\n",
    "            ti, yi, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "\n",
    "    ys_to_plot = jax.vmap(_interp)(ts_to_plot, ys_to_plot)[..., 0]\n",
    "    ys_sampled = jax.vmap(generator)(\n",
    "        ts_to_plot, key=jrandom.split(sample_key, num_samples)\n",
    "    )[..., 0]\n",
    "    kwargs = dict(label=\"Real\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_to_plot):\n",
    "        ax.plot(ti, yi, c=\"dodgerblue\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    kwargs = dict(label=\"Generated\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_sampled):\n",
    "        ax.plot(ti, yi, c=\"crimson\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    ax.set_title(f\"{num_samples} samples from both real and generated distributions.\")\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"neural_sde_samples.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
