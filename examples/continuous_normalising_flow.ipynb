{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944a9162-1fa3-487d-96dc-4cc8e42568d3",
   "metadata": {},
   "source": [
    "# Continuous Normalising Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c3e90-8793-4f50-a387-1c9a3c0f187e",
   "metadata": {},
   "source": [
    "This example is a bit of fun! It constructs a continuous normalising flow (CNF) to learn a distribution specified by a (greyscale) image. That is, the target distribution is over R^2, and the image specifies the (unnormalised) density at each point.\n",
    "\n",
    "You can specify your own images, and learn your own flows.\n",
    "\n",
    "Some example outputs from this script:\n",
    "\n",
    "![cat](../imgs/target-progression.png)\n",
    "![cat](../imgs/cat-progression.png)\n",
    "![cat](../imgs/butterfly-progression.png)\n",
    "\n",
    "Reference:\n",
    "\n",
    "```bibtex\n",
    "@article{grathwohl2019ffjord,\n",
    "    title={{FFJORD}: {F}ree-form {C}ontinuous {D}ynamics for {S}calable {R}eversible\n",
    "           {G}enerative {M}odels},\n",
    "    author={Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and\n",
    "            Sutskever, Ilya and Duvenaud, David},\n",
    "    journal={International Conference on Learning Representations},\n",
    "    year={2019},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c298e7-bc2c-4426-b4d4-4351d314ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import imageio\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "here = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9019ed7-5fed-4719-84dc-a5915a66d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_log_likelihood(y):\n",
    "    return -0.5 * (y.size * math.log(2 * math.pi) + jnp.sum(y ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8cf55-9060-4940-a513-37a5a0906fb3",
   "metadata": {},
   "source": [
    "Use Hutchinson's trace estimator to estimate the divergence of the vector field.\n",
    "(As introduced in FFJORD.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f1eaa-1a5d-4a28-8f48-23d7913c71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_logp_wrapper(t, y, args):\n",
    "    y, _ = y\n",
    "    *args, eps, func = args\n",
    "    fn = lambda y: func(t, y, args)\n",
    "    f, vjp_fn = jax.vjp(fn, y)\n",
    "    (eps_dfdy,) = vjp_fn(eps)\n",
    "    logp = jnp.sum(eps_dfdy * eps)\n",
    "    return f, logp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c665db5-7516-41c3-958f-9c55ed9d5297",
   "metadata": {},
   "source": [
    "Alternatively, compute the divergence exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a739011-8f84-487d-80ef-9884cb045f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_logp_wrapper(t, y, args):\n",
    "    y, _ = y\n",
    "    *args, _, func = args\n",
    "    fn = lambda y: func(t, y, args)\n",
    "    f, vjp_fn = jax.vjp(fn, y)\n",
    "    (size,) = y.shape  # this implementation only works for 1D input\n",
    "    eye = jnp.eye(size)\n",
    "    (dfdy,) = jax.vmap(vjp_fn)(eye)\n",
    "    logp = jnp.trace(dfdy)\n",
    "    return f, logp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1679a-b4ea-43d8-806a-f8d0e2cd6d06",
   "metadata": {},
   "source": [
    "Credit: this layer, and some of the default hyperparameters below, are taken from the FFJORD repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99379798-330e-4b1d-838e-3e818b0a5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatSquash(eqx.Module):\n",
    "    lin1: eqx.nn.Linear\n",
    "    lin2: eqx.nn.Linear\n",
    "    lin3: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, *, in_size, out_size, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        key1, key2, key3 = jrandom.split(key, 3)\n",
    "        self.lin1 = eqx.nn.Linear(in_size, out_size, key=key1)\n",
    "        self.lin2 = eqx.nn.Linear(1, out_size, key=key2)\n",
    "        self.lin3 = eqx.nn.Linear(1, out_size, use_bias=False, key=key3)\n",
    "\n",
    "    def __call__(self, t, y):\n",
    "        return self.lin1(y) * jnn.sigmoid(self.lin2(t)) + self.lin3(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34873810-ec45-4fb7-a22e-d701ffd7e2f4",
   "metadata": {},
   "source": [
    "Basically just an MLP, using tanh as the activation function and ConcatSquash instead of linear layers.\n",
    "\n",
    "This is the vector field on the right hand side of the ODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb75d86-7a02-48bd-b909-c6561a9e0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(self, *, data_size, width_size, depth, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        keys = jrandom.split(key, depth + 1)\n",
    "        layers = []\n",
    "        if depth == 0:\n",
    "            layers.append(\n",
    "                ConcatSquash(in_size=data_size, out_size=data_size, key=keys[0])\n",
    "            )\n",
    "        else:\n",
    "            layers.append(\n",
    "                ConcatSquash(in_size=data_size, out_size=width_size, key=keys[0])\n",
    "            )\n",
    "            for i in range(depth - 1):\n",
    "                layers.append(\n",
    "                    ConcatSquash(\n",
    "                        in_size=width_size, out_size=width_size, key=keys[i + 1]\n",
    "                    )\n",
    "                )\n",
    "            layers.append(\n",
    "                ConcatSquash(in_size=width_size, out_size=data_size, key=keys[-1])\n",
    "            )\n",
    "        self.layers = layers\n",
    "\n",
    "    @jax.jit\n",
    "    def __call__(self, t, y, args):\n",
    "        t = jnp.asarray(t)[None]\n",
    "        for layer in self.layers[:-1]:\n",
    "            y = layer(t, y)\n",
    "            y = jnn.tanh(y)\n",
    "        y = self.layers[-1](t, y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf0fc2-be07-44a9-8c03-7861c603887a",
   "metadata": {},
   "source": [
    "Wrap up the differential equation solve into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08547fa-72d2-4f2e-a80e-0bcc982617db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNF(eqx.Module):\n",
    "    funcs: List[Func]\n",
    "    data_size: int\n",
    "    exact_logp: bool\n",
    "    t0: float\n",
    "    t1: float\n",
    "    dt0: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        data_size,\n",
    "        exact_logp,\n",
    "        num_blocks,\n",
    "        width_size,\n",
    "        depth,\n",
    "        key,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        keys = jrandom.split(key, num_blocks)\n",
    "        self.funcs = [\n",
    "            Func(\n",
    "                data_size=data_size,\n",
    "                width_size=width_size,\n",
    "                depth=depth,\n",
    "                key=k,\n",
    "            )\n",
    "            for k in keys\n",
    "        ]\n",
    "        self.data_size = data_size\n",
    "        self.exact_logp = exact_logp\n",
    "        self.t0 = 0.0\n",
    "        self.t1 = 0.5\n",
    "        self.dt0 = 0.05\n",
    "\n",
    "    # Runs backward-in-time to train the CNF.\n",
    "    def train(self, y, *, key):\n",
    "        solver = diffrax.tsit5(\n",
    "            exact_logp_wrapper if self.exact_logp else approx_logp_wrapper\n",
    "        )\n",
    "        eps = jrandom.normal(key, y.shape)\n",
    "        delta_log_likelihood = 0.0\n",
    "        for func in reversed(self.funcs):\n",
    "            y = (y, delta_log_likelihood)\n",
    "            sol = diffrax.diffeqsolve(\n",
    "                solver, self.t1, self.t0, y, -self.dt0, (eps, func)\n",
    "            )\n",
    "            (y,), (delta_log_likelihood,) = sol.ys\n",
    "        return delta_log_likelihood + normal_log_likelihood(y)\n",
    "\n",
    "    # Runs forward-in-time to draw samples from the CNF.\n",
    "    def sample(self, *, key):\n",
    "        y = jrandom.normal(key, (self.data_size,))\n",
    "        for func in self.funcs:\n",
    "            solver = diffrax.tsit5(func)\n",
    "            sol = diffrax.diffeqsolve(solver, self.t0, self.t1, y, self.dt0)\n",
    "            (y,) = sol.ys\n",
    "        return y\n",
    "\n",
    "    # By way of illustration, we have a variant sample method we can query to see the\n",
    "    # evolution of the samples during the forward solve.\n",
    "    def sample_flow(self, *, key):\n",
    "        t_so_far = self.t0\n",
    "        t_end = self.t0 + (self.t1 - self.t0) * len(self.funcs)\n",
    "        save_times = jnp.linspace(self.t0, t_end, 6)\n",
    "        y = jrandom.normal(key, (self.data_size,))\n",
    "        out = []\n",
    "        for i, func in enumerate(self.funcs):\n",
    "            if i == len(self.funcs) - 1:\n",
    "                save_ts = save_times[t_so_far <= save_times] - t_so_far\n",
    "            else:\n",
    "                save_ts = (\n",
    "                    save_times[\n",
    "                        (t_so_far <= save_times)\n",
    "                        & (save_times < t_so_far + self.t1 - self.t0)\n",
    "                    ]\n",
    "                    - t_so_far\n",
    "                )\n",
    "                t_so_far = t_so_far + self.t1 - self.t0\n",
    "            saveat = diffrax.SaveAt(ts=save_ts)\n",
    "            solver = diffrax.tsit5(func)\n",
    "            sol = diffrax.diffeqsolve(\n",
    "                solver, self.t0, self.t1, y, self.dt0, saveat=saveat\n",
    "            )\n",
    "            out.append(sol.ys)\n",
    "            y = sol.ys[-1]\n",
    "        out = jnp.concatenate(out)\n",
    "        assert len(out) == 6  # number of points we saved at\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381db08-a0ca-453e-9c76-55bc02ef9dd4",
   "metadata": {},
   "source": [
    "Converts the input image into data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f976be5-aad4-41a8-93f7-e82bdeb343fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    # integer array of shape (height, width, channels) with values in {0, ..., 255}\n",
    "    img = jnp.asarray(imageio.imread(path))\n",
    "    if img.shape[-1] == 4:\n",
    "        img = img[..., :-1]  # ignore alpha channel\n",
    "    height, width, channels = img.shape\n",
    "    assert channels == 3\n",
    "    # Convert to greyscale for simplicity.\n",
    "    img = img @ jnp.array([0.2989, 0.5870, 0.1140])\n",
    "    img = jnp.transpose(img)[:, ::-1]  # (width, height)\n",
    "    x = jnp.arange(width, dtype=jnp.float32)\n",
    "    y = jnp.arange(height, dtype=jnp.float32)\n",
    "    x, y = jnp.broadcast_arrays(x[:, None], y[None, :])\n",
    "    weights = 1 - img.reshape(-1).astype(jnp.float32) / jnp.max(img)\n",
    "    dataset = jnp.stack(\n",
    "        [x.reshape(-1), y.reshape(-1)], axis=-1\n",
    "    )  # shape (dataset_size, 2)\n",
    "    # For efficiency we don't bother with the particles that will have weight zero.\n",
    "    cond = img.reshape(-1) < 254\n",
    "    dataset = dataset[cond]\n",
    "    weights = weights[cond]\n",
    "    mean = jnp.mean(dataset, axis=0)\n",
    "    std = jnp.std(dataset, axis=0) + 1e-6\n",
    "    dataset = (dataset - mean) / std\n",
    "\n",
    "    return dataset, weights, mean, std, img, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bd95d-3497-4ef2-834f-34bcf7158967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530bfb5-00b6-4164-890d-d3cb7d9839a7",
   "metadata": {},
   "source": [
    "Bring everything together. This function is our entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78370bd3-1280-4f27-9fe6-0c1543924701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    in_path,\n",
    "    out_path=None,\n",
    "    batch_size=500,\n",
    "    virtual_batches=2,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    steps=10000,\n",
    "    exact_logp=True,\n",
    "    num_blocks=2,\n",
    "    width_size=64,\n",
    "    depth=3,\n",
    "    seed=5678,\n",
    "):\n",
    "    if out_path is None:\n",
    "        out_path = here / \"cnf_results\" / pathlib.Path(in_path).name\n",
    "    out_path = pathlib.Path(out_path)\n",
    "    out_path.resolve().parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    model_key, loader_key, train_key, sample_key = jrandom.split(key, 4)\n",
    "\n",
    "    dataset, weights, mean, std, img, width, height = get_data(in_path)\n",
    "    dataset_size, data_size = dataset.shape\n",
    "    data_iter = iter(dataloader((dataset, weights), batch_size, key=loader_key))\n",
    "\n",
    "    model = CNF(\n",
    "        data_size=data_size,\n",
    "        exact_logp=exact_logp,\n",
    "        num_blocks=num_blocks,\n",
    "        width_size=width_size,\n",
    "        depth=depth,\n",
    "        key=model_key,\n",
    "    )\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss(model, data, weight, key):\n",
    "        batch_size, _ = data.shape\n",
    "        noise_key, train_key = jrandom.split(key, 2)\n",
    "        train_key = jrandom.split(key, batch_size)\n",
    "        data = data + jrandom.normal(noise_key, data.shape) * 0.5 / std\n",
    "        log_likelihood = jax.vmap(model.train)(data, key=train_key)\n",
    "        return -jnp.mean(weight * log_likelihood)\n",
    "\n",
    "    optim = optax.adamw(lr, weight_decay=weight_decay)\n",
    "    opt_state = optim.init(\n",
    "        jax.tree_map(lambda x: x if eqx.is_inexact_array(x) else None, model)\n",
    "    )\n",
    "\n",
    "    for step in range(steps):\n",
    "        start = time.time()\n",
    "        value = 0\n",
    "        grads = jax.tree_map(lambda _: 0.0, model)\n",
    "        for _ in range(virtual_batches):\n",
    "            data, weight = next(data_iter)\n",
    "            value_, grads_ = loss(model, data, weight, train_key)\n",
    "            (train_key,) = jrandom.split(train_key, 1)\n",
    "            value = value + value_\n",
    "            grads = jax.tree_map(\n",
    "                lambda a, b: None if b is None else a + b, grads, grads_\n",
    "            )\n",
    "        value = value / virtual_batches\n",
    "        grads = jax.tree_map(lambda a: a / virtual_batches, grads)\n",
    "        end = time.time()\n",
    "\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        print(f\"Step: {step}, Loss: {value}, Computation time: {end - start}\")\n",
    "\n",
    "    print(f\"Best value: {value}\")\n",
    "    num_samples = 5000\n",
    "    sample_key = jrandom.split(sample_key, num_samples)\n",
    "    samples = jax.vmap(model.sample)(key=sample_key)\n",
    "    sample_flows = jax.vmap(model.sample_flow, out_axes=-1)(key=sample_key)\n",
    "    fig, (*axs, ax, axtrue) = plt.subplots(\n",
    "        1,\n",
    "        2 + len(sample_flows),\n",
    "        figsize=((2 + len(sample_flows)) * 10 * height / width, 10),\n",
    "    )\n",
    "\n",
    "    samples = samples * std + mean\n",
    "    x = samples[:, 0]\n",
    "    y = samples[:, 1]\n",
    "    ax.scatter(x, y, c=\"black\", s=2)\n",
    "    ax.set_xlim(-0.5, width - 0.5)\n",
    "    ax.set_ylim(-0.5, height - 0.5)\n",
    "    ax.set_aspect(height / width)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    axtrue.imshow(img.T, origin=\"lower\", cmap=\"gray\")\n",
    "    axtrue.set_aspect(height / width)\n",
    "    axtrue.set_xticks([])\n",
    "    axtrue.set_yticks([])\n",
    "\n",
    "    x_resolution = 100\n",
    "    y_resolution = int(x_resolution * (height / width))\n",
    "    sample_flows = sample_flows * std[:, None] + mean[:, None]\n",
    "    x_pos, y_pos = jnp.broadcast_arrays(\n",
    "        jnp.linspace(-1, width + 1, x_resolution)[:, None],\n",
    "        jnp.linspace(-1, height + 1, y_resolution)[None, :],\n",
    "    )\n",
    "    positions = jnp.stack([jnp.ravel(x_pos), jnp.ravel(y_pos)])\n",
    "    densities = [stats.gaussian_kde(samples)(positions) for samples in sample_flows]\n",
    "    for i, (ax, density) in enumerate(zip(axs, densities)):\n",
    "        density = jnp.reshape(density, (x_resolution, y_resolution))\n",
    "        ax.imshow(density.T, origin=\"lower\", cmap=\"plasma\")\n",
    "        ax.set_aspect(height / width)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96907221-1911-4dc6-9573-637c9d4fb6b9",
   "metadata": {},
   "source": [
    "And now the following commands will reproduce the images displayed at the start.\n",
    "\n",
    "```python\n",
    "main(in_path=\"../imgs/cat.png\")\n",
    "main(in_path=\"../imgs/butterfly.png\", num_blocks=3)\n",
    "main(in_path=\"../imgs/target.png\", width_size=128)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
