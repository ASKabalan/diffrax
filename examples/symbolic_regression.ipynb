{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce28f2b-008b-4beb-ba51-5c478e4bdec7",
   "metadata": {},
   "source": [
    "# Symbolic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a09d3-f78c-4b46-acd9-3a4250bb34c9",
   "metadata": {},
   "source": [
    "This example combines neural differential equations with regularised evolution to discover the equations\n",
    "\n",
    "$\\frac{\\mathrm{d} x}{\\mathrm{d} t}(t) = \\frac{y(t)}{1 + y(t)}$\n",
    "\n",
    "$\\frac{\\mathrm{d} y}{\\mathrm{d} t}(t) = \\frac{-x(t)}{1 + x(t)}$\n",
    "\n",
    "directly from data.\n",
    "\n",
    "**References:**\n",
    "\n",
    "This example appears as an example in:\n",
    "\n",
    "```bibtex\n",
    "@phdthesis{kidger2021on,\n",
    "    title={{O}n {N}eural {D}ifferential {E}quations},\n",
    "    author={Patrick Kidger},\n",
    "    year={2021},\n",
    "    school={University of Oxford},\n",
    "}\n",
    "```\n",
    "\n",
    "Whilst drawing heavy inspiration from:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{cranmer2020discovering,\n",
    "    title={{D}iscovering {S}ymbolic {M}odels from {D}eep {L}earning with {I}nductive\n",
    "           {B}iases},\n",
    "    author={Cranmer, Miles and Sanchez Gonzalez, Alvaro and Battaglia, Peter and\n",
    "            Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},\n",
    "    booktitle={Advances in Neural Information Processing Systems},\n",
    "    publisher={Curran Associates, Inc.},\n",
    "    year={2020},\n",
    "}\n",
    "\n",
    "@software{cranmer2020pysr,\n",
    "    title={PySR: Fast \\& Parallelized Symbolic Regression in Python/Julia},\n",
    "    author={Miles Cranmer},\n",
    "    publisher={Zenodo},\n",
    "    url={http://doi.org/10.5281/zenodo.4041459},\n",
    "    year={2020},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea04fa4-a95b-47f8-b0eb-be297037bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tempfile\n",
    "from typing import List\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import pysr\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44a335-c3c3-41cb-b2ca-23c21cb195b5",
   "metadata": {},
   "source": [
    "This example extends the neural ODE example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee3b4d-565d-42b4-a5b2-43b2a078bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run neural_ode.ipynb\n",
    "neural_ode_main = main  # get the main function from the other example.  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26c41f-7682-4ad0-aa33-77e22b2768f8",
   "metadata": {},
   "source": [
    "Now for a bunch of helpers. We'll use these in a moment; skip over them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d294688-3ea9-43ce-855f-cc587de908a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantise(expr, quantise_to):\n",
    "    if isinstance(expr, sympy.Float):\n",
    "        return expr.func(round(float(expr) / quantise_to) * quantise_to)\n",
    "    elif isinstance(expr, sympy.Symbol):\n",
    "        return expr\n",
    "    else:\n",
    "        return expr.func(*[quantise(arg, quantise_to) for arg in expr.args])\n",
    "\n",
    "\n",
    "class SymbolicFn(eqx.Module):\n",
    "    fn: callable\n",
    "    parameters: jnp.ndarray\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Dummy batch/unbatching. PySR assumes its JAX'd symbolic functions act on\n",
    "        # tensors with a single batch dimension.\n",
    "        return jnp.squeeze(self.fn(x[None], self.parameters))\n",
    "\n",
    "\n",
    "class Stack(eqx.Module):\n",
    "    modules: List[eqx.Module]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.stack([module(x) for module in self.modules], axis=-1)\n",
    "\n",
    "\n",
    "def expr_size(expr):\n",
    "    return sum(expr_size(v) for v in expr.args) + 1\n",
    "\n",
    "\n",
    "def _replace_parameters(expr, parameters, i_ref):\n",
    "    if isinstance(expr, sympy.Float):\n",
    "        i_ref[0] += 1\n",
    "        return expr.func(parameters[i_ref[0]])\n",
    "    elif isinstance(expr, sympy.Symbol):\n",
    "        return expr\n",
    "    else:\n",
    "        return expr.func(\n",
    "            *[_replace_parameters(arg, parameters, i_ref) for arg in expr.args]\n",
    "        )\n",
    "\n",
    "\n",
    "def replace_parameters(expr, parameters):\n",
    "    i_ref = [-1]  # Distinctly sketchy approach to making this conversion.\n",
    "    return _replace_parameters(expr, parameters, i_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35245e23-17e0-4c7e-bb60-3e462b9b3b3c",
   "metadata": {},
   "source": [
    "Now for the main program, which we can run with `main()`. We discuss what's happening at each step in the comments -- read on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0525ef3-979f-4cf5-b15f-530eae7b8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    neural_dataset_size=256,\n",
    "    neural_batch_size=32,\n",
    "    neural_lr=3e-3,\n",
    "    neural_steps=5000,\n",
    "    neural_width_size=64,\n",
    "    neural_depth=2,\n",
    "    symbolic_dataset_size=2000,\n",
    "    symbolic_num_populations=100,\n",
    "    symbolic_population_size=20,\n",
    "    symbolic_migration_steps=10,\n",
    "    symbolic_mutation_steps=50,\n",
    "    symbolic_descent_steps=50,\n",
    "    pareto_coefficient=2,\n",
    "    fine_tuning_steps=500,\n",
    "    fine_tuning_lr=3e-3,\n",
    "    quantise_to=0.01,\n",
    "    seed=5678,\n",
    "):\n",
    "    # First obtain a neural approximation to the dynamics.\n",
    "    ts, ys, model, _ = neural_ode_main(\n",
    "        dataset_size=neural_dataset_size,\n",
    "        batch_size=neural_batch_size,\n",
    "        lr=neural_lr,\n",
    "        steps=neural_steps,\n",
    "        width_size=neural_width_size,\n",
    "        depth=neural_depth,\n",
    "        seed=seed,\n",
    "        plot=False,\n",
    "    )\n",
    "\n",
    "    # Now symbolically regress across the learnt vector field, to obtain a Pareto\n",
    "    # frontier of symbolic equations, that trade loss against complexity of the\n",
    "    # equation.\n",
    "    vector_field = model.solver.term.vector_field.impl\n",
    "    dataset_size, length_size, data_size = ys.shape\n",
    "    in_ = ys.reshape(dataset_size * length_size, data_size)[:symbolic_dataset_size]\n",
    "    out = jax.vmap(vector_field)(in_)\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        pareto_frontier = pysr.pysr(\n",
    "            in_,\n",
    "            out,\n",
    "            niterations=symbolic_migration_steps,\n",
    "            ncyclesperiteration=symbolic_mutation_steps,\n",
    "            populations=symbolic_num_populations,\n",
    "            npop=symbolic_population_size,\n",
    "            optimizer_iterations=symbolic_descent_steps,\n",
    "            optimizer_nrestarts=1,\n",
    "            procs=1,\n",
    "            tempdir=tempdir,\n",
    "            temp_equation_file=True,\n",
    "            output_jax_format=True,\n",
    "        )\n",
    "\n",
    "    # We now select the `best' equation from this frontier.\n",
    "    # PySR actually has a built-in way of doing this (`parsimony`) if you want.\n",
    "    expressions = []\n",
    "    symbolic_fns = []\n",
    "    for pareto_frontier_i, out_i in zip(pareto_frontier, jnp.rollaxis(out, 1)):\n",
    "        best_expression = None\n",
    "        best_symbolic_fn = None\n",
    "        best_expr_size = None\n",
    "        best_expr_value = None\n",
    "        for expr in pareto_frontier_i.itertuples():\n",
    "            symbolic_fn = SymbolicFn(\n",
    "                expr.jax_format[\"callable\"], expr.jax_format[\"parameters\"]\n",
    "            )\n",
    "            loss = jnp.mean((jax.vmap(symbolic_fn)(in_) - out_i) ** 2)\n",
    "            if best_expression is None:\n",
    "                best_expression = expr.sympy_format\n",
    "                best_symbolic_fn = symbolic_fn\n",
    "                best_expr_size = expr_size(expr.sympy_format)\n",
    "                best_expr_value = math.log(loss, pareto_coefficient) + best_expr_size\n",
    "            else:\n",
    "                _expr_size = expr_size(expr.sympy_format)\n",
    "                expr_value = math.log(loss, pareto_coefficient) + _expr_size\n",
    "                if expr_value < best_expr_value or (\n",
    "                    (expr_value == best_expr_value) and (_expr_size < best_expr_size)\n",
    "                ):\n",
    "                    best_expression = expr.sympy_format\n",
    "                    best_symbolic_fn = symbolic_fn\n",
    "                    best_expr_size = _expr_size\n",
    "                    best_expr_value = expr_value\n",
    "        expressions.append(best_expression)\n",
    "        symbolic_fns.append(best_symbolic_fn)\n",
    "\n",
    "    # Now the constants in this expression have been optimised for regressing across\n",
    "    # the neural vector field. This was good enough to obtain the symbolic expression,\n",
    "    # but won't quite be perfect -- some of the constants will be slightly off.\n",
    "    #\n",
    "    # To fix this we now plug our symbolic function back into the original (neural)\n",
    "    # model and apply gradient descent.\n",
    "    symbolic_fn = Stack(symbolic_fns)\n",
    "    symbolic_model = eqx.tree_at(\n",
    "        lambda m: m.solver.term.vector_field.impl,\n",
    "        model,\n",
    "        symbolic_fn,\n",
    "        replace_subtree=True,\n",
    "    )\n",
    "\n",
    "    @eqx.filter_grad\n",
    "    def grad_loss(symbolic_model):\n",
    "        vmap_model = jax.vmap(symbolic_model, in_axes=(None, 0))\n",
    "        pred_ys = vmap_model(ts, ys[:, 0])\n",
    "        return jnp.mean((ys - pred_ys) ** 2)\n",
    "\n",
    "    optim = optax.adam(fine_tuning_lr)\n",
    "    opt_state = optim.init(eqx.filter(symbolic_model, eqx.is_array))\n",
    "    for _ in range(fine_tuning_steps):\n",
    "        grads = grad_loss(symbolic_model)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        symbolic_model = eqx.apply_updates(symbolic_model, updates)\n",
    "\n",
    "    # Finally we round each constant to the nearest multiple of `quantise_to`.\n",
    "    trained_expressions = []\n",
    "    for module, expression in zip(\n",
    "        symbolic_model.solver.term.vector_field.impl.modules, expressions\n",
    "    ):\n",
    "        expression = replace_parameters(expression, module.parameters.tolist())\n",
    "        expression = quantise(expression, quantise_to)\n",
    "        trained_expressions.append(expression)\n",
    "\n",
    "    print(f\"Expressions found: {trained_expressions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
