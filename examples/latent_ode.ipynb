{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36b21f0-6445-4b28-bc68-b04895b68f61",
   "metadata": {},
   "source": [
    "# Latent ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f5f6b-5b03-414a-b37a-ccaca860224b",
   "metadata": {},
   "source": [
    "This example trains a [Latent ODE](https://arxiv.org/abs/1810.01367).\n",
    "\n",
    "In this case, it's on a simple dataset of decaying oscillators. That is, 2-dimensional time series that look like:\n",
    "\n",
    "```\n",
    "xx    ***\n",
    "    **   *\n",
    "  x*      **\n",
    "  *x\n",
    "    x       *\n",
    " *           *                  xxxxx\n",
    "*    x        *               xx    xx *******\n",
    "                             x        x       **\n",
    "      x        *            x        * x        *                  xxxxxxxx  ******\n",
    "       x        *          x        *   x        *              xxx       *xx      *\n",
    "                          x        *     xx       **           x        **   xx\n",
    "        x        *       x        *        x        *        xx       **       xx\n",
    "                  *     x        *          x        **     x        *           xxx\n",
    "         x         *            *            x         *  xx       **\n",
    "          x         *  x       *              xx        xx*     ***\n",
    "           x         *x       *                 xxx  xxx   *****\n",
    "            x        x*      *                     xx\n",
    "             x     xx  ******\n",
    "              xxxxx\n",
    "```\n",
    "              \n",
    "The model is trained to generate samples that look like this.\n",
    "\n",
    "What's really nice about this example is that we will take the underlying data to be irregularly sampled. We will have different observation times for different batch elements.\n",
    "\n",
    "Most differential equation libraries will struggle with this, as they usually mandate that the differential equation be solved over the same timespan for all batch elements. Working around this can involve programming complexity like outputting at lots and lots of times (the union of all the observations times in the batch), or mathematical complexities like reparameterising the differentiating equation.\n",
    "\n",
    "However Diffrax is capable of handling this without such issues! You can `vmap` over\n",
    "different integration times for different batch elements.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "```bibtex\n",
    "@incollection{rubanova2019latent,\n",
    "    title={{L}atent {O}rdinary {D}ifferential {E}quations for {I}rregularly-{S}ampled\n",
    "           {T}ime {S}eries},\n",
    "    author={Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David K.},\n",
    "    booktitle={Advances in Neural Information Processing Systems},\n",
    "    publisher={Curran Associates, Inc.},\n",
    "    year={2019},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7f1a8-c288-4ea8-b751-622a6d76f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})\n",
    "here = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a86a99-786d-4f4a-8376-71955dcff0f6",
   "metadata": {},
   "source": [
    "The vector field. Note its overall structure of `scalar * tanh(mlp(y))` which is a good structure for Latent ODEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc61000-f4b6-4693-92eb-1d93dfbddf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    scale: jnp.ndarray\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.scale * self.mlp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7004ef1-e03b-4c20-9c16-53266d4b3c15",
   "metadata": {},
   "source": [
    "Wrap up the differential equation solve into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466e77f-c7f2-4b32-8145-a92d2ca7911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODE(eqx.Module):\n",
    "    solver: diffrax.AbstractSolver\n",
    "    rnn_cell: eqx.nn.GRUCell\n",
    "\n",
    "    hidden_to_latent: eqx.nn.Linear\n",
    "    latent_to_hidden: eqx.nn.MLP\n",
    "    hidden_to_data: eqx.nn.Linear\n",
    "\n",
    "    hidden_size: int\n",
    "    latent_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self, *, data_size, hidden_size, latent_size, width_size, depth, key, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        mkey, gkey, hlkey, lhkey, hdkey = jrandom.split(key, 5)\n",
    "\n",
    "        scale = jnp.ones(())\n",
    "        mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mkey,\n",
    "        )\n",
    "        self.solver = diffrax.dopri5(Func(scale, mlp))\n",
    "        self.rnn_cell = eqx.nn.GRUCell(data_size + 1, hidden_size, key=gkey)\n",
    "\n",
    "        self.hidden_to_latent = eqx.nn.Linear(hidden_size, 2 * latent_size, key=hlkey)\n",
    "        self.latent_to_hidden = eqx.nn.MLP(\n",
    "            latent_size, hidden_size, width_size=width_size, depth=depth, key=lhkey\n",
    "        )\n",
    "        self.hidden_to_data = eqx.nn.Linear(hidden_size, data_size, key=hdkey)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    # Encoder of the VAE\n",
    "    @eqx.filter_jit\n",
    "    def _latent(self, ts, ys, key):\n",
    "        data = jnp.concatenate([ts[:, None], ys], axis=1)\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "        for data_i in reversed(data):\n",
    "            hidden = self.rnn_cell(data_i, hidden)\n",
    "        context = self.hidden_to_latent(hidden)\n",
    "        mean, logstd = context[: self.latent_size], context[self.latent_size :]\n",
    "        std = jnp.exp(logstd)\n",
    "        latent = mean + jrandom.normal(key, (self.latent_size,)) * std\n",
    "        return latent, mean, std\n",
    "\n",
    "    # Decoder of the VAE\n",
    "    def _sample(self, ts, latent):\n",
    "        y0 = self.latent_to_hidden(latent)\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            self.solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            y0,\n",
    "            dt0=0.4,  # selected as a reasonable choice for this problem\n",
    "            saveat=diffrax.SaveAt(ts=ts),\n",
    "        )\n",
    "        return jax.vmap(self.hidden_to_data)(sol.ys)\n",
    "\n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def _loss(ys, pred_ys, mean, std):\n",
    "        # -log p_θ with Gaussian p_θ\n",
    "        reconstruction_loss = 0.5 * jnp.sum((ys - pred_ys) ** 2)\n",
    "        # KL(N(mean, std^2) || N(0, 1))\n",
    "        variational_loss = 0.5 * jnp.sum(mean ** 2 + std ** 2 - 2 * jnp.log(std) - 1)\n",
    "        return reconstruction_loss + variational_loss\n",
    "\n",
    "    # Run both encoder and decoder during training.\n",
    "    def train(self, ts, ys, *, key):\n",
    "        latent, mean, std = self._latent(ts, ys, key)\n",
    "        pred_ys = self._sample(ts, latent)\n",
    "        return self._loss(ys, pred_ys, mean, std)\n",
    "\n",
    "    # Run just the decoder during inference.\n",
    "    def sample(self, ts, *, key):\n",
    "        latent = jrandom.normal(key, (self.latent_size,))\n",
    "        return self._sample(ts, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e7035-eaaa-4a1f-b2fc-c69b3f5ebb73",
   "metadata": {},
   "source": [
    "Toy dataset of decaying oscillators.\n",
    "\n",
    "By way of illustration we set this up as a differential equaiton and solve this using Diffrax as well. (Despite this being an autonomous linear ODE, for which a closed-form solution is actually available.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21ca14-d588-45d0-b512-653114a00b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ykey, tkey1, tkey2 = jrandom.split(key, 3)\n",
    "\n",
    "    y0 = jrandom.normal(ykey, (dataset_size, 2))\n",
    "\n",
    "    t0 = 0\n",
    "    t1 = 2 + jrandom.uniform(tkey1, (dataset_size,))\n",
    "    ts = jrandom.uniform(tkey2, (dataset_size, 20)) * (t1[:, None] - t0) + t0\n",
    "    ts = jnp.sort(ts)\n",
    "\n",
    "    def func(t, y, args):\n",
    "        return jnp.array([[-0.1, 1.3], [-1, -0.1]]) @ y\n",
    "\n",
    "    def solve(ts, y0):\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            diffrax.tsit5(func),\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            y0,\n",
    "            dt0=0.1,\n",
    "            saveat=diffrax.SaveAt(ts=ts),\n",
    "        )\n",
    "        return sol.ys\n",
    "\n",
    "    ys = jax.vmap(solve)(ts, y0)\n",
    "\n",
    "    return ts, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6a004-0930-4a04-9139-677be3f6787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3fca6-c5cc-4c8a-950b-9ccabaa34efb",
   "metadata": {},
   "source": [
    "The main entry point. Try running `main()` to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c4793-6699-48e6-8a88-4874c4b77592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    out_path=here / \"latent_ode.png\",\n",
    "    dataset_size=10000,\n",
    "    batch_size=256,\n",
    "    lr=1e-2,\n",
    "    steps=250,\n",
    "    save_every=50,\n",
    "    hidden_size=16,\n",
    "    latent_size=16,\n",
    "    width_size=16,\n",
    "    depth=2,\n",
    "    seed=5678,\n",
    "):\n",
    "    out_path = str(out_path)\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    data_key, model_key, loader_key, train_key, sample_key = jrandom.split(key, 5)\n",
    "\n",
    "    ts, ys = get_data(dataset_size, key=data_key)\n",
    "\n",
    "    model = LatentODE(\n",
    "        data_size=ys.shape[-1],\n",
    "        hidden_size=hidden_size,\n",
    "        latent_size=latent_size,\n",
    "        width_size=width_size,\n",
    "        depth=depth,\n",
    "        key=model_key,\n",
    "    )\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss(model, ts_i, ys_i, *, key_i):\n",
    "        batch_size, _ = ts_i.shape\n",
    "        key_i = jrandom.split(key_i, batch_size)\n",
    "        loss = jax.vmap(model.train)(ts_i, ys_i, key=key_i)\n",
    "        return jnp.mean(loss)\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(\n",
    "        jax.tree_map(lambda leaf: leaf if eqx.is_inexact_array(leaf) else None, model)\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    num_plots = 1 + (steps - 1) // save_every\n",
    "    if ((steps - 1) % save_every) != 0:\n",
    "        num_plots += 1\n",
    "    fig, axs = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n",
    "    axs[0].set_ylabel(\"x\")\n",
    "    axs = iter(axs)\n",
    "    for step, (ts_i, ys_i) in zip(\n",
    "        range(steps), dataloader((ts, ys), batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        value, grads = loss(model, ts_i, ys_i, key_i=train_key)\n",
    "        end = time.time()\n",
    "\n",
    "        (train_key,) = jrandom.split(train_key, 1)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        print(f\"Step: {step}, Loss: {value}, Computation time: {end - start}\")\n",
    "\n",
    "        if (step % save_every) == 0 or step == steps - 1:\n",
    "            ax = next(axs)\n",
    "            # Sample over a longer time interval than we trained on. The model will be\n",
    "            # sufficiently good that it will correctly extrapolate!\n",
    "            sample_t = jnp.linspace(0, 12, 300)\n",
    "            sample_y = model.sample(sample_t, key=sample_key)\n",
    "            sample_t = np.asarray(sample_t)\n",
    "            sample_y = np.asarray(sample_y)\n",
    "            ax.plot(sample_t, sample_y[:, 0])\n",
    "            ax.plot(sample_t, sample_y[:, 1])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel(\"t\")\n",
    "\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
