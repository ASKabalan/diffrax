{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2826ad0-230e-4085-ac05-49b819d6fbb5",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901cdecb-fd5e-4f52-9e4d-83aad8dbb09e",
   "metadata": {},
   "source": [
    "Training a small neural network by SGD, using Diffrax.\n",
    "\n",
    "Stochastic gradient descent is just the Euler approximation to gradient flow. Recall the differential equation for gradient flow:\n",
    "\n",
    "$\\frac{\\mathrm{d} \\theta}{\\mathrm{d} t}(t) = -\\nabla_\\theta f(\\theta(t))$\n",
    "\n",
    "where $f(\\theta(t))$ is the loss evaluated on the value of the parameters at timestep $t$ of training. Then the explicit Euler method produces:\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t + \\Delta t * (-\\nabla_\\theta f(\\theta(t)))$\n",
    "\n",
    "which is the familiar formula for (stochastic) gradient descent. The step size $\\Delta t$ corresponds to the learning rate.\n",
    "\n",
    "In this example we do exactly this to train a small neural network using diffrax. Practically speaking you should probably just use a standard optimisation library, but doing it via numerical differential equation solvers can be a bit of fun.\n",
    "\n",
    "For example you can switch out the Euler method for other methods like Heun, or you can start using adaptive step sizes (learning rates), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23dd52-a1c0-4692-8faa-8702ce4d9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "from diffrax import diffeqsolve, euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee9831-2ac8-4b8e-b455-bdd56096374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(key, dataset_size):\n",
    "    x = jrandom.normal(key, (dataset_size, 1))\n",
    "    y = 5 * x - 2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908517a3-66a8-4b94-8965-92dcf0222551",
   "metadata": {},
   "source": [
    "Functionally pure dataloader.\n",
    "\n",
    "- Wraps some state (arrays, key, batch_size).\n",
    "- Produces a pseudorandom batch of data every time it is called. (In this case random sampling with replacement. A more sophisticated implementatation could follow normal iteration-over-dataset behaviour.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bad26-5eee-4023-8c39-08443a7a3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(eqx.Module):\n",
    "    arrays: Tuple[jnp.ndarray]\n",
    "    batch_size: int\n",
    "    key: jrandom.PRNGKey\n",
    "\n",
    "    # Equinox Modules are Python dataclasses, which allow for a __post_init__.\n",
    "    def __post_init__(self):\n",
    "        dataset_size = self.arrays[0].shape[0]\n",
    "        assert all(array.shape[0] == dataset_size for array in self.arrays)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        key = jrandom.fold_in(self.key, step)\n",
    "        dataset_size = self.arrays[0].shape[0]\n",
    "        batch_indices = jrandom.randint(\n",
    "            key, (self.batch_size,), minval=0, maxval=dataset_size\n",
    "        )\n",
    "        return tuple(array[batch_indices] for array in self.arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975bbc9b-a764-4699-ae82-28caa892f43f",
   "metadata": {},
   "source": [
    "Finally run `main()` to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1362e-3596-4dd3-acf3-bfd585554f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=10000,\n",
    "    batch_size=256,\n",
    "    learning_rate=3e-3,\n",
    "    steps=1000,\n",
    "    width_size=8,\n",
    "    seed=56789,\n",
    "    jit=False,\n",
    "    printout=True,\n",
    "):\n",
    "    start = time.time()\n",
    "    data_key, model_key, loader_key = jrandom.split(jrandom.PRNGKey(seed), 3)\n",
    "\n",
    "    data = get_data(data_key, dataset_size)\n",
    "    dataloader = DataLoader(data, batch_size, key=loader_key)\n",
    "\n",
    "    # Equinox models are PyTrees of both JAX arrays and arbitrary Python objects. (e.g.\n",
    "    # activation functions.)\n",
    "    # Diffrax's diffeqsolve only works on PyTrees of JAX arrays, so we need to split up\n",
    "    # these two pieces.\n",
    "    model = eqx.nn.MLP(\n",
    "        in_size=1, out_size=1, width_size=width_size, depth=1, key=model_key\n",
    "    )\n",
    "    params, static = eqx.partition(model, eqx.is_inexact_array)\n",
    "\n",
    "    # Define the vector field as the gradient of a loss function.\n",
    "    @jax.jit\n",
    "    @jax.value_and_grad\n",
    "    def loss(params, x, y):\n",
    "        model = eqx.combine(params, static)\n",
    "        pred_y = jax.vmap(model)(x)\n",
    "        return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "    def vector_field(step, params, _):\n",
    "        x, y = dataloader(step)\n",
    "        value, grad = loss(params, x, y)\n",
    "        if printout:\n",
    "            print(step, value)\n",
    "        return jax.tree_map(lambda g: -learning_rate * g, grad)\n",
    "\n",
    "    # Try running this with jit=True/False. You should probably see a ~2-3x speedup\n",
    "    # using JIT.\n",
    "    #\n",
    "    # Note that we can safely JIT because vector_field has only benign side-effects,\n",
    "    # namely the print statement. (Doing so will mean that we don't get to see the\n",
    "    # print statements as training progresses, however.)\n",
    "    #\n",
    "    # In particular this uses the fact that the dataloader is functionally pure, and\n",
    "    # doesn't maintain any internal state.\n",
    "    solution = diffeqsolve(\n",
    "        euler(vector_field), t0=0, t1=steps, y0=params, dt0=1, jit=jit\n",
    "    )\n",
    "\n",
    "    params = jax.tree_map(lambda x: x[0], solution.ys)\n",
    "    value, _ = loss(params, *dataloader(0))\n",
    "    end = time.time()\n",
    "    print(f\"Final loss: {value}\")\n",
    "    print(f\"Training completed in {end - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
